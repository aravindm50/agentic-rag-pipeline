{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fcc5e8",
   "metadata": {},
   "source": [
    "# Agentic Rag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b257f3a",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 1. Setup & Imports\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally, uncomment to install dependencies.\n",
    "# !pip install --upgrade pip\n",
    "!python -m pip install pinecone sentence-transformers langchain-pinecone langchain langchain-huggingface langchain-google-genai --quiet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74df6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import pinecone\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from pydantic import BaseModel\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"agentic_rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64aec8",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 2. Config\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2aabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LOCATION = \"us-central1\"\n",
    "GEMINI_MODEL = \"gemini-2.5-flash\"   # For generation\n",
    "GEMINI_EMBED_MODEL = \"models/embedding-001\"\n",
    "\n",
    "INDEX_NAME = \"tredence-aravind\"\n",
    "\n",
    "# Init clients\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"Set PINECONE_API_KEY environment variable\")\n",
    "\n",
    "client = genai.Client()\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41668c77",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 3. Load KB & Build Index\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77d6b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:agentic_rag:Loaded 30 KB entries\n"
     ]
    }
   ],
   "source": [
    "with open(\"self_critique_loop_dataset.json\") as f:\n",
    "    kb = json.load(f)\n",
    "\n",
    "logger.info(f\"Loaded {len(kb)} KB entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08896fa1",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 4. Embedding Function\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b196636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    resp = client.models.embed_content(model=GEMINI_EMBED_MODEL, contents=texts)\n",
    "    # Each element has .values for embedding\n",
    "    return [e.values for e in resp.embeddings]\n",
    "\n",
    "# Check embedding dimension\n",
    "sample_vec = embed_texts([kb[0][\"answer_snippet\"]])[0]\n",
    "dim = len(sample_vec)\n",
    "logger.info(f\"Embedding dimension: {dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5726b8",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 5. Create Pinecone Index\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7233b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zadmin/genai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if not pc.has_index(INDEX_NAME):\n",
    "    pc.create_index(name=INDEX_NAME, dimension=dim, metric=\"cosine\",spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "\n",
    "index = pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd912b83",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 6. Upsert KB\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f5871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:KB indexed successfully.\n"
     ]
    }
   ],
   "source": [
    "for entry in kb:\n",
    "    vec = embed_texts([entry[\"answer_snippet\"]])[0]\n",
    "    index.upsert([(entry[\"doc_id\"], vec, entry)])\n",
    "logger.info(\"KB indexed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6b0c9",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 7. LangGraph Workflow\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c9b0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(BaseModel):\n",
    "    question: str\n",
    "    snippets: List[Dict]\n",
    "    initial_answer: str = \"\"\n",
    "    critique: str = \"\"\n",
    "    final_answer: str = \"\"\n",
    "    missing_keywords: Optional[List[str]] = []\n",
    "\n",
    "def retrieve_kb(state: RAGState) -> RAGState:\n",
    "    q_vec = embed_texts([state.question])[0]\n",
    "    res = index.query(vector=q_vec, top_k=5, include_metadata=True)\n",
    "    state.snippets = [m[\"metadata\"] for m in res[\"matches\"]]\n",
    "    logger.info(f\"Retrieved {len(state.snippets)} snippets: {[s['doc_id'] for s in state.snippets]}\")\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: RAGState) -> RAGState:\n",
    "    context = \"\\n\".join([f\"[{s['doc_id']}] {s['answer_snippet']}\" for s in state.snippets])\n",
    "    prompt = f\"Question: {state.question}\\nContext:\\n{context}\\nAnswer with citations [KBxxx]:\"\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0\n",
    "        ),\n",
    "        contents=[prompt]\n",
    "    )\n",
    "    state.initial_answer = resp.text\n",
    "    logger.info(f\"Generated initial answer:\\n{state.initial_answer}\")\n",
    "    return state\n",
    "\n",
    "def critique_answer(state: RAGState) -> RAGState:\n",
    "    # prompt = (\n",
    "    #     f\"Critique the following answer for completeness vs the KB snippets.\\n\\n\"\n",
    "    #     f\"Answer: {state.initial_answer}\\n\\n\"\n",
    "    #     f\"Context:\\n\" + \"\\n\".join([f\"[{s['doc_id']}] {s['answer_snippet']}\" for s in state.snippets]) +\n",
    "    #     \"\\n\\nRespond with 'COMPLETE' if sufficient, or 'REFINE: <missing keywords>' if something is missing.\"\n",
    "    # )\n",
    "    prompt = (f\"\"\"\n",
    "    You are a reviewer. \n",
    "    - Question: {state.question}\n",
    "    - Initial Answer: {state.initial_answer}\n",
    "    - Available KB Snippets: \\n \"\"\" + \"\\n\".join([f\"[{s['doc_id']}] {s['answer_snippet']}\" for s in state.snippets]) +\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Check: \n",
    "    1. Does the answer fully address the user’s question? \n",
    "    2. Does it cover all relevant snippets?\n",
    "    3. Is the user question relevant to the documents provided?\n",
    "\n",
    "    If yes → respond with ONLY 'COMPLETE'.\n",
    "    If no → respond with 'REFINE: <list of missing topics/keywords>'.\n",
    "    \"\"\")\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0\n",
    "        ),\n",
    "        contents=[prompt]\n",
    "    )\n",
    "    critique_text = resp.text.strip()\n",
    "    state.critique = critique_text\n",
    "    logger.info(f\"Critique: {state.critique}\")\n",
    "\n",
    "    if critique_text.upper().startswith(\"REFINE:\"):\n",
    "        payload = critique_text.split(\"REFINE:\")[1].strip()\n",
    "        state.missing_keywords = [k.strip() for k in payload.split(\",\") if k.strip()]\n",
    "\n",
    "    return state\n",
    "\n",
    "def refine_answer(state: RAGState) -> RAGState:\n",
    "    if not hasattr(state, \"missing_keywords\") or not state.missing_keywords:\n",
    "        state.final_answer = state.initial_answer\n",
    "        return state\n",
    "\n",
    "    # Retrieve additional snippet for missing keyword\n",
    "    keyword = state.missing_keywords[0]\n",
    "    q_vec = embed_texts([keyword])[0]\n",
    "    res = index.query(vector=q_vec, top_k=1, include_metadata=True)\n",
    "    new_snippet = res[\"matches\"][0][\"metadata\"]\n",
    "    state.snippets.append(new_snippet)\n",
    "    logger.info(f\"Retrieved extra snippet [{new_snippet['doc_id']}] for refinement.\")\n",
    "\n",
    "    # Regenerate answer with additional snippet\n",
    "    context = \"\\n\".join([f\"[{s['doc_id']}] {s['answer_snippet']}\" for s in state.snippets])\n",
    "    prompt = f\"Question: {state.question}\\nContext:\\n{context}\\nProvide a refined answer with citations [KBxxx]:\"\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0\n",
    "        ),\n",
    "        contents=[prompt]\n",
    "    )\n",
    "    state.final_answer = resp.text\n",
    "    logger.info(f\"Refined answer:\\n{state.final_answer}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61295683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LangGraph\n",
    "graph = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"retriever\", retrieve_kb)\n",
    "graph.add_node(\"generator\", generate_answer)\n",
    "graph.add_node(\"critic\", critique_answer)\n",
    "graph.add_node(\"refiner\", refine_answer)\n",
    "\n",
    "# Connect START → retriever\n",
    "graph.add_edge(START, \"retriever\")\n",
    "\n",
    "# Linear flow: retriever → generator → critic\n",
    "graph.add_edge(\"retriever\", \"generator\")\n",
    "graph.add_edge(\"generator\", \"critic\")\n",
    "\n",
    "# Conditional edge from critic\n",
    "graph.add_conditional_edges(\"critic\", lambda state: \"refiner\" if \"REFINE\" in state.critique else END)\n",
    "\n",
    "# After refinement, go to END\n",
    "graph.add_edge(\"refiner\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45e5ed",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 8. Run Queries\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f073deb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:agentic_rag:Running pipeline for: What are best practices for caching?\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved 5 snippets: ['KB003', 'KB023', 'KB013', 'KB012', 'KB002']\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Generated initial answer:\n",
      "When addressing caching, it's important to follow well-defined patterns [KB003, KB023, KB013].\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Critique: REFINE: specific caching best practices\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved extra snippet [KB023] for refinement.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Refined answer:\n",
      "A key best practice for caching is to follow well-defined patterns [KB003, KB013, KB023].\n",
      "INFO:agentic_rag:Running pipeline for: How should I set up CI/CD pipelines?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are best practices for caching?\n",
      "Answer:\n",
      "When addressing caching, it's important to follow well-defined patterns [KB003, KB023, KB013].\n",
      "Critique: REFINE: specific caching best practices\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved 5 snippets: ['KB007', 'KB027', 'KB017', 'KB016', 'KB006']\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Generated initial answer:\n",
      "When setting up CI/CD pipelines, it's important to follow well-defined patterns [KB007, KB027, KB017].\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Critique: REFINE: specific patterns for CI/CD setup, steps to set up CI/CD pipelines, components of CI/CD pipelines, best practices for CI/CD implementation\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved extra snippet [KB027] for refinement.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Refined answer:\n",
      "When setting up CI/CD pipelines, it is important to follow well-defined patterns [KB007, KB017, KB027].\n",
      "INFO:agentic_rag:Running pipeline for: What are performance tuning tips?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: How should I set up CI/CD pipelines?\n",
      "Answer:\n",
      "When setting up CI/CD pipelines, it's important to follow well-defined patterns [KB007, KB027, KB017].\n",
      "Critique: REFINE: specific patterns for CI/CD setup, steps to set up CI/CD pipelines, components of CI/CD pipelines, best practices for CI/CD implementation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved 5 snippets: ['KB002', 'KB022', 'KB012', 'KB013', 'KB003']\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Generated initial answer:\n",
      "When addressing performance tuning, it's important to follow well-defined patterns [KB002, KB022, KB012].\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Critique: REFINE: caching\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved extra snippet [KB023] for refinement.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Refined answer:\n",
      "When addressing performance tuning, it's important to follow well-defined patterns [KB002, KB022, KB012].\n",
      "INFO:agentic_rag:Running pipeline for: How do I version my APIs?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are performance tuning tips?\n",
      "Answer:\n",
      "When addressing performance tuning, it's important to follow well-defined patterns [KB002, KB022, KB012].\n",
      "Critique: REFINE: caching\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved 5 snippets: ['KB005', 'KB025', 'KB015', 'KB020', 'KB010']\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Generated initial answer:\n",
      "When addressing API versioning, it's important to follow well-defined patterns [KB005, KB025, KB015].\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Critique: REFINE: API versioning methods/strategies\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved extra snippet [KB025] for refinement.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Refined answer:\n",
      "When versioning your APIs, it is crucial to follow well-defined patterns [KB005, KB015, KB025].\n",
      "INFO:agentic_rag:Running pipeline for: What should I consider for error handling?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: How do I version my APIs?\n",
      "Answer:\n",
      "When addressing API versioning, it's important to follow well-defined patterns [KB005, KB025, KB015].\n",
      "Critique: REFINE: API versioning methods/strategies\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:agentic_rag:Retrieved 5 snippets: ['KB009', 'KB029', 'KB019', 'KB018', 'KB008']\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Generated initial answer:\n",
      "When addressing error handling, it's important to follow well-defined patterns [KB009, KB029, KB019].\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:agentic_rag:Critique: COMPLETE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What should I consider for error handling?\n",
      "Answer:\n",
      "When addressing error handling, it's important to follow well-defined patterns [KB009, KB029, KB019].\n",
      "Critique: COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"What are best practices for caching?\",\n",
    "    \"How should I set up CI/CD pipelines?\",\n",
    "    \"What are performance tuning tips?\",\n",
    "    \"How do I version my APIs?\",\n",
    "    \"What should I consider for error handling?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    logger.info(f\"Running pipeline for: {q}\")\n",
    "    state = app.invoke(RAGState(question=q, snippets=[]))\n",
    "    print(f\"\\nQ: {q}\\nAnswer:\\n{state['initial_answer']}\\nCritique: {state['critique']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3fb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
